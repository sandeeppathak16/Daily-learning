{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "id": "Soum7ahm0z5gNgw1jFIBD",
      "type": "text",
      "x": 921.5621165136071,
      "y": 2277.508299304248,
      "width": 1732.4990234375,
      "height": 7275,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b2h",
      "roundness": null,
      "seed": 1776937088,
      "version": 2617,
      "versionNonce": 918196096,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1756111459788,
      "link": null,
      "locked": false,
      "text": "Design Sequencer\n\nMotivation \n\n    there can be millions of events happening per second in large distributed system. commenting a post on facebook, sharing a tweet, and posting a picture\n     on instagram are just few example of such events. we need a mechanism to distinguish these events from each other. one such mechanism is the assignment of \n    globally unique ids to each of these events.\n\n    Assigning a primary key to an entry in a database is another use case of a unique ID. Usually, the auto-increment feature in databases fulfills this requirement.\n     However, that feature won’t work for a distributed database, where different nodes independently generate the identifiers. For this case, we need a unique \n    ID generator that acts as a primary key in a distributed setting—for example, a horizontally-sharded table.\n\n\nDesign of a Unique ID Generator \n\n    requirements for unique identifiers\n\n    - uniqueness \n    - scalability \n    - availability\n    - 64 bit numeric id - we restrict the length to 64 bits because this bit size is enough for many years in the future.\n\n    let's calculate the number of years after which our id range will be wrap around\n\n    total available number = 2^64 = 1.8446744 * 10^19 \n    total number of events per day = 10^9\n    number of events in a year = 365 * 10^9 \n    number of years to consume identifier range = 2^64 / (365 * 10^9) = 50,539,024.8595 years\n\nFirst solution is UUID\n\n    it is 128 bit long id, each server can generate its own id and assign the id to its respective event. no coordination is needed for uuid \n     since it's independent of the server. scaling up and down is easy with uuid and this system is also highly available. \n\n    cons of uuid is that it is 128 bit long and using 128 bit long as primary key is slow the index hence slow inserts. there is chance of duplicate\n\nSecond solution is using database\n\n    - let's try mimicking the auto increment feature of a database. consider a center database that provide a current id and then increase the value by one.\n    we can use current id as unique identifier for our events.\n\n    to handle single point of failure we could use n node and increase the value by n. this method is scalable and prevent duplicate ids. Though this method is \n    somewhat scalable, it’s difficult to scale for multiple data centers. The task of adding and removing a server can result in duplicate IDs. \n    For example, suppose m=3, and server A generates the unique IDs 1, 4, and 7. Server B generates the IDs 2, 5, and 8, while server C generates the \n    IDs 3, 6, and 9. Server B faces downtime due to some failure. Now, the value m is updated to 2. Server A generates 9 as its following unique ID, but this ID \n    has already been generated by server C. Therefore, the IDs aren’t unique anymore.\n\nThird solution using a range handler\n\n     - Let’s try to overcome the problems identified in the previous methods. We can use ranges in a central server. Suppose we have multiple ranges for one to two \n        billion, such as 1 to 1,000,000; 1,000,001 to 2,000,000; and so on. In such a case, a central microservice can provide a range to a server upon request.\n\n        Any server can claim a range when it needs it for the first time or if it runs out of the range. Suppose a server has a range, and now it keeps the start \n        of the range in a local variable. Whenever a request for an ID is made, it provides the local variable value to the requestor and increments the value by one.\n        \n        Let’s say server 1 claims the number range 300,001 to 400,000. After this range claim, the user ID 300,001 is assigned to the first request. The server then returns \n        300,002 to the next user, incrementing its current position within the range. This continues until user ID 400,000 is released by the server. The application server then \n        queries the central server for the next available range and repeats this process.\n        \n        This resolves the problem of the duplication of user IDs. Each application server can respond to requests concurrently. We can add a load balancer over a set of servers \n        to mitigate the load of requests.\n\n        We use a microservice called range handler that keeps a record of all the taken and available ranges. The status of each range can determine if a range is available \n        or not. The state—that is, which server has what range assigned to it—can be saved on a replicated storage.\n\n        This microservice can become a single point of failure, but a failover server acts as the savior in that case. The failover server hands out ranges when the \n        main server is down. We can recover the state of available and unavailable ranges from the latest checkpoint of the replicated store.\n        \n\nUnique IDs with causality\n\n    - apart from having unique identifiers for events, we are also interested in finding the sequence of these events.\n\n    Using UNIX Timestamps for ID Generation\n\n        Granularity: UNIX timestamps are precise to the millisecond, making them suitable for distinguishing different events.\n\n    ID Generation Server:\n\n        A dedicated server generates one unique ID per millisecond.\n    \n        Any request for a new ID is routed to this server.\n    \n        The server returns a timestamp, which is then used to produce the unique ID.\n\n    Throughput:\n    \n        Since one ID can be generated per millisecond, that equals 1,000 IDs per second.\n        \n        Daily capacity calculation: 24 (hours) × 60 (minutes/hour)× 60(seconds/minute) ×1000(IDs/second) = 86, 400,000 IDs/day\n    \n    Scale: That’s ~86 million IDs per day, less than one billion per day.\n\n\nTwitter Snowflake (64-bit ID)\n\n     ├─ Sign Bit (1 bit)\n     │    • Always 0\n     │    • Ensures positive number\n     │\n     ├─ Timestamp (41 bits)\n     │    • Milliseconds since custom epoch\n     │    • Default epoch: Nov 4, 2010, 01:42:54 UTC\n     │    • Example: Jan 1, 2022 → custom epoch = 0\n     │    • Valid for ~69 years before overflow\n     │\n     ├─ Worker ID (10 bits)\n     │    • Identifies machine / process\n     │    • Range: 2^10 = 1024 workers\n     │\n     └─ Sequence Number (12 bits)\n          • Incremented for each ID in same millisecond\n          • Range: 2^12 = 4096 per millisecond\n          • Resets to 0 after 4096\n\n    Pros\n        • Time-ordered IDs (sortable)\n        • Highly available & distributed\n        • Large ID space\n    \n    Cons\n        • Dead periods waste IDs (gaps)\n        • Reliance on system time → NTP drift issues\n        • Clock skew can break causality\n\nUsing Logical Clocks\n\n    - Lamport clocks\n\n        Each node keeps an integer counter L (start 0)\n\n        Before any local event: L ← L + 1\n\n        On send(message): attach L\n        \n        On receive(message with Lm): L ← max(L, Lm) + 1\n\n        Lamport — Example (A → B)\n    \n            A local evt: A.L=1\n            \n            A sends msg with 1\n            \n            B receives (B.L=0) ⇒ B.L=max(0,1)+1=2\n            \n            Order: A.evt ⟶ (message) ⟶ B.evt\n    \n        Lamport — What It Guarantees\n    \n            Monotonic partial ordering (happened‑before)\n            \n            Can force total order by (L, nodeId) tiebreak\n            \n            Cannot detect concurrency reliably\n    \n        Lamport — Pros / Cons\n        \n            Very lightweight, simple, scalable\n            – No causal history; concurrent events indistinguishable\n        \n        Lamport — Snowflake‑Style ID\n        [LamportClockBits][WorkerID][Sequence]\n        \n            Monotonic without wall clock\n            \n            Still no explicit causality/concurrency info\n\n    - Vector Clock \n\n        Each node keeps vector V[1..n]\n        \n        Before local event at i: V[i]←V[i]+1\n        \n        On send: attach full V\n        \n        On receive(Vm): V[k]←max(V[k],Vm[k]) ∀k, then V[i]←V[i]+1\n        \n        Vector — Compare Two Events\n        \n            X ≤ Y iff ∀k: X[k] ≤ Y[k]\n            \n            X < Y iff X ≤ Y and X ≠ Y (X happened‑before Y)\n        \n            Concurrent iff neither X ≤ Y nor Y ≤ X\n        \n        Vector — Example (3 nodes A,B,C)\n        \n            A local: A.V=[1,0,0]\n            \n            A→B sends [1,0,0]\n            \n            B recv: max([0,0,0],[1,0,0])=[1,0,0]; then B bumps: [1,1,0]\n\n            C local twice: [0,0,2] (independent)\n            \n            [1,1,0] and [0,0,2] are concurrent\n\n        Vector — What It Guarantees\n        \n            Captures causal history\n            \n            Distinguishes concurrency explicitly\n        \n        Vector — Pros / Cons\n        \n            Precise causality + concurrency detection\n            – Vector size grows with #nodes (storage/overhead)\n        \n        Vector — Snowflake‑Style ID\n        [VectorClockBits][WorkerID]\n        \n            Strong ordering semantics\n            \n            Bit budget rises with participating nodes\n\n    - TrueTime\n\n        Not a single timestamp.\n        Returns a time INTERVAL: [TE, TL]\n        • TE = earliest possible current time\n        • TL = latest possible current time\n        • ε (epsilon) = TL - TE (uncertainty window)\n\n\n        Why Intervals? (Problem it solves)\n        \n            Physical clocks drift → wrong ordering.\n            With intervals:\n            If A.latest < B.earliest ⇒ A definitely before B.\n            If intervals overlap ⇒ order unknown (possibly concurrent).\n            \n        \n        How Google Keeps ε Small\n        \n            • Time masters in every data center\n            • Some have GPS receivers, some have atomic clocks\n            • Regular sync to servers\n            • Typical ε ≈ 6–10 ms (often ~7 ms)\n            \n        \n        TrueTime API (mental model)\n        \n            TT.now() → { earliest: TE, latest: TL }\n            TT.after(t)  → true if t < TE\n            TT.before(t) → true if TL < t\n            Use these to reason about definite ordering.\n\n\n        Spanner Commit-Wait (intuition)\n        \n            To make a write \"globally in the past\":\n            1) Choose a commit timestamp Tc within TrueTime.\n            2) Wait until TT.after(Tc) is true (commit-wait).\n            3) Now everyone will see Tc as already happened.\n        \n        \n        Build a 64-bit ID with TrueTime\n        \n            [ TE(41 bits, ms) | ε(4 bits) | WorkerID(10 bits) | Seq(8 bits) ]\n            \n            • TE: earliest bound (time-sortable, safer than wall-clock)\n            • ε: store uncertainty (0–15 ms range)\n            • WorkerID: up to 1024 workers\n            • Seq: up to 256 IDs per ms per worker\n\n\n            Example ID (readable form)\n            \n            TE = 1699999999123 ms\n            ε  = 7\n            WorkerID = 42\n            Seq = 5\n            ID = [1699999999123 | 7 | 42 | 5]\n\n\n        Pros\n            \n            • Global uniqueness with ordering guarantees\n            • If intervals don't overlap → strict causality\n            • Monotonic, time-sortable IDs without relying on perfect clocks\n            • Scales across DCs\n            \n        \n        Cons / Caveats\n        \n            • Overlapping intervals ⇒ order not guaranteed (may be concurrent)\n            • Infra cost: GPS + atomic clocks + monitori\n\n\n    \n    ",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Design Sequencer\n\nMotivation \n\n    there can be millions of events happening per second in large distributed system. commenting a post on facebook, sharing a tweet, and posting a picture\n     on instagram are just few example of such events. we need a mechanism to distinguish these events from each other. one such mechanism is the assignment of \n    globally unique ids to each of these events.\n\n    Assigning a primary key to an entry in a database is another use case of a unique ID. Usually, the auto-increment feature in databases fulfills this requirement.\n     However, that feature won’t work for a distributed database, where different nodes independently generate the identifiers. For this case, we need a unique \n    ID generator that acts as a primary key in a distributed setting—for example, a horizontally-sharded table.\n\n\nDesign of a Unique ID Generator \n\n    requirements for unique identifiers\n\n    - uniqueness \n    - scalability \n    - availability\n    - 64 bit numeric id - we restrict the length to 64 bits because this bit size is enough for many years in the future.\n\n    let's calculate the number of years after which our id range will be wrap around\n\n    total available number = 2^64 = 1.8446744 * 10^19 \n    total number of events per day = 10^9\n    number of events in a year = 365 * 10^9 \n    number of years to consume identifier range = 2^64 / (365 * 10^9) = 50,539,024.8595 years\n\nFirst solution is UUID\n\n    it is 128 bit long id, each server can generate its own id and assign the id to its respective event. no coordination is needed for uuid \n     since it's independent of the server. scaling up and down is easy with uuid and this system is also highly available. \n\n    cons of uuid is that it is 128 bit long and using 128 bit long as primary key is slow the index hence slow inserts. there is chance of duplicate\n\nSecond solution is using database\n\n    - let's try mimicking the auto increment feature of a database. consider a center database that provide a current id and then increase the value by one.\n    we can use current id as unique identifier for our events.\n\n    to handle single point of failure we could use n node and increase the value by n. this method is scalable and prevent duplicate ids. Though this method is \n    somewhat scalable, it’s difficult to scale for multiple data centers. The task of adding and removing a server can result in duplicate IDs. \n    For example, suppose m=3, and server A generates the unique IDs 1, 4, and 7. Server B generates the IDs 2, 5, and 8, while server C generates the \n    IDs 3, 6, and 9. Server B faces downtime due to some failure. Now, the value m is updated to 2. Server A generates 9 as its following unique ID, but this ID \n    has already been generated by server C. Therefore, the IDs aren’t unique anymore.\n\nThird solution using a range handler\n\n     - Let’s try to overcome the problems identified in the previous methods. We can use ranges in a central server. Suppose we have multiple ranges for one to two \n        billion, such as 1 to 1,000,000; 1,000,001 to 2,000,000; and so on. In such a case, a central microservice can provide a range to a server upon request.\n\n        Any server can claim a range when it needs it for the first time or if it runs out of the range. Suppose a server has a range, and now it keeps the start \n        of the range in a local variable. Whenever a request for an ID is made, it provides the local variable value to the requestor and increments the value by one.\n        \n        Let’s say server 1 claims the number range 300,001 to 400,000. After this range claim, the user ID 300,001 is assigned to the first request. The server then returns \n        300,002 to the next user, incrementing its current position within the range. This continues until user ID 400,000 is released by the server. The application server then \n        queries the central server for the next available range and repeats this process.\n        \n        This resolves the problem of the duplication of user IDs. Each application server can respond to requests concurrently. We can add a load balancer over a set of servers \n        to mitigate the load of requests.\n\n        We use a microservice called range handler that keeps a record of all the taken and available ranges. The status of each range can determine if a range is available \n        or not. The state—that is, which server has what range assigned to it—can be saved on a replicated storage.\n\n        This microservice can become a single point of failure, but a failover server acts as the savior in that case. The failover server hands out ranges when the \n        main server is down. We can recover the state of available and unavailable ranges from the latest checkpoint of the replicated store.\n        \n\nUnique IDs with causality\n\n    - apart from having unique identifiers for events, we are also interested in finding the sequence of these events.\n\n    Using UNIX Timestamps for ID Generation\n\n        Granularity: UNIX timestamps are precise to the millisecond, making them suitable for distinguishing different events.\n\n    ID Generation Server:\n\n        A dedicated server generates one unique ID per millisecond.\n    \n        Any request for a new ID is routed to this server.\n    \n        The server returns a timestamp, which is then used to produce the unique ID.\n\n    Throughput:\n    \n        Since one ID can be generated per millisecond, that equals 1,000 IDs per second.\n        \n        Daily capacity calculation: 24 (hours) × 60 (minutes/hour)× 60(seconds/minute) ×1000(IDs/second) = 86, 400,000 IDs/day\n    \n    Scale: That’s ~86 million IDs per day, less than one billion per day.\n\n\nTwitter Snowflake (64-bit ID)\n\n     ├─ Sign Bit (1 bit)\n     │    • Always 0\n     │    • Ensures positive number\n     │\n     ├─ Timestamp (41 bits)\n     │    • Milliseconds since custom epoch\n     │    • Default epoch: Nov 4, 2010, 01:42:54 UTC\n     │    • Example: Jan 1, 2022 → custom epoch = 0\n     │    • Valid for ~69 years before overflow\n     │\n     ├─ Worker ID (10 bits)\n     │    • Identifies machine / process\n     │    • Range: 2^10 = 1024 workers\n     │\n     └─ Sequence Number (12 bits)\n          • Incremented for each ID in same millisecond\n          • Range: 2^12 = 4096 per millisecond\n          • Resets to 0 after 4096\n\n    Pros\n        • Time-ordered IDs (sortable)\n        • Highly available & distributed\n        • Large ID space\n    \n    Cons\n        • Dead periods waste IDs (gaps)\n        • Reliance on system time → NTP drift issues\n        • Clock skew can break causality\n\nUsing Logical Clocks\n\n    - Lamport clocks\n\n        Each node keeps an integer counter L (start 0)\n\n        Before any local event: L ← L + 1\n\n        On send(message): attach L\n        \n        On receive(message with Lm): L ← max(L, Lm) + 1\n\n        Lamport — Example (A → B)\n    \n            A local evt: A.L=1\n            \n            A sends msg with 1\n            \n            B receives (B.L=0) ⇒ B.L=max(0,1)+1=2\n            \n            Order: A.evt ⟶ (message) ⟶ B.evt\n    \n        Lamport — What It Guarantees\n    \n            Monotonic partial ordering (happened‑before)\n            \n            Can force total order by (L, nodeId) tiebreak\n            \n            Cannot detect concurrency reliably\n    \n        Lamport — Pros / Cons\n        \n            Very lightweight, simple, scalable\n            – No causal history; concurrent events indistinguishable\n        \n        Lamport — Snowflake‑Style ID\n        [LamportClockBits][WorkerID][Sequence]\n        \n            Monotonic without wall clock\n            \n            Still no explicit causality/concurrency info\n\n    - Vector Clock \n\n        Each node keeps vector V[1..n]\n        \n        Before local event at i: V[i]←V[i]+1\n        \n        On send: attach full V\n        \n        On receive(Vm): V[k]←max(V[k],Vm[k]) ∀k, then V[i]←V[i]+1\n        \n        Vector — Compare Two Events\n        \n            X ≤ Y iff ∀k: X[k] ≤ Y[k]\n            \n            X < Y iff X ≤ Y and X ≠ Y (X happened‑before Y)\n        \n            Concurrent iff neither X ≤ Y nor Y ≤ X\n        \n        Vector — Example (3 nodes A,B,C)\n        \n            A local: A.V=[1,0,0]\n            \n            A→B sends [1,0,0]\n            \n            B recv: max([0,0,0],[1,0,0])=[1,0,0]; then B bumps: [1,1,0]\n\n            C local twice: [0,0,2] (independent)\n            \n            [1,1,0] and [0,0,2] are concurrent\n\n        Vector — What It Guarantees\n        \n            Captures causal history\n            \n            Distinguishes concurrency explicitly\n        \n        Vector — Pros / Cons\n        \n            Precise causality + concurrency detection\n            – Vector size grows with #nodes (storage/overhead)\n        \n        Vector — Snowflake‑Style ID\n        [VectorClockBits][WorkerID]\n        \n            Strong ordering semantics\n            \n            Bit budget rises with participating nodes\n\n    - TrueTime\n\n        Not a single timestamp.\n        Returns a time INTERVAL: [TE, TL]\n        • TE = earliest possible current time\n        • TL = latest possible current time\n        • ε (epsilon) = TL - TE (uncertainty window)\n\n\n        Why Intervals? (Problem it solves)\n        \n            Physical clocks drift → wrong ordering.\n            With intervals:\n            If A.latest < B.earliest ⇒ A definitely before B.\n            If intervals overlap ⇒ order unknown (possibly concurrent).\n            \n        \n        How Google Keeps ε Small\n        \n            • Time masters in every data center\n            • Some have GPS receivers, some have atomic clocks\n            • Regular sync to servers\n            • Typical ε ≈ 6–10 ms (often ~7 ms)\n            \n        \n        TrueTime API (mental model)\n        \n            TT.now() → { earliest: TE, latest: TL }\n            TT.after(t)  → true if t < TE\n            TT.before(t) → true if TL < t\n            Use these to reason about definite ordering.\n\n\n        Spanner Commit-Wait (intuition)\n        \n            To make a write \"globally in the past\":\n            1) Choose a commit timestamp Tc within TrueTime.\n            2) Wait until TT.after(Tc) is true (commit-wait).\n            3) Now everyone will see Tc as already happened.\n        \n        \n        Build a 64-bit ID with TrueTime\n        \n            [ TE(41 bits, ms) | ε(4 bits) | WorkerID(10 bits) | Seq(8 bits) ]\n            \n            • TE: earliest bound (time-sortable, safer than wall-clock)\n            • ε: store uncertainty (0–15 ms range)\n            • WorkerID: up to 1024 workers\n            • Seq: up to 256 IDs per ms per worker\n\n\n            Example ID (readable form)\n            \n            TE = 1699999999123 ms\n            ε  = 7\n            WorkerID = 42\n            Seq = 5\n            ID = [1699999999123 | 7 | 42 | 5]\n\n\n        Pros\n            \n            • Global uniqueness with ordering guarantees\n            • If intervals don't overlap → strict causality\n            • Monotonic, time-sortable IDs without relying on perfect clocks\n            • Scales across DCs\n            \n        \n        Cons / Caveats\n        \n            • Overlapping intervals ⇒ order not guaranteed (may be concurrent)\n            • Infra cost: GPS + atomic clocks + monitori\n\n\n    \n    ",
      "autoResize": true,
      "lineHeight": 1.25
    }
  ],
  "appState": {
    "gridSize": 20,
    "gridStep": 5,
    "gridModeEnabled": false,
    "viewBackgroundColor": "#ffffff",
    "lockedMultiSelections": {}
  },
  "files": {}
}